<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>sds-3.x/ScaDaMaLe</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/student-project-18_group-ProjectRL/00_Problem_Description.html">00_Problem_Description</a></li><li class="chapter-item expanded affix "><a href="contents/student-project-18_group-ProjectRL/01_The_ALS_method.html">01_The_ALS_method</a></li><li class="chapter-item expanded affix "><a href="contents/student-project-18_group-ProjectRL/02_Extensions.html">02_Extensions</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="recommender-system"><a class="header" href="#recommender-system">Recommender System</a></h1>
<p><strong>Project members</strong>: - Ines De Miranda De Matos Louren√ßo - Yassir Jedra - Filippo Vannella</p>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>In this project, we develop and analyse a Recommendation System.</p>
<p>We guide ourselves by the file 036<em>ALS</em>MovieRecommender, which provides an introduction to recommendation systems.</p>
<p>The goal of recommendation systems is to predict the preferences of users based on their similarity to other users.</p>
</div>
<div class="cell markdown">
<h2 id="problem-description"><a class="header" href="#problem-description">Problem description</a></h2>
<p>The recommender problem is the following: we have N users and M movies, and a rating matrix R where each component \(r_{ij}\) corresponds to the rating given by user i to the movie j. The problem is that some components of R are missing, meaning that not all users have rated all movies, and the objective is to estimate these ratings to know which movies to recommend to the users.</p>
<p>To do this we will use a method called Alternating Least Squares (ALS), which estimates a Ratings matrix based on a User and a Movies matrices.</p>
</div>
<div class="cell markdown">
<h2 id="contents-and-contributions"><a class="header" href="#contents-and-contributions">Contents and contributions</a></h2>
<p>We divide the work in the following parts, each of which in a different notebook:</p>
<ul>
<li>
<p>00_Problem description: In this present notebook we introduce what is a recommendation system and which problems it tries to solve. We finally present the datasets used for this project, which include the original (small) data set, and a Netflix (large) dataset.</p>
</li>
<li>
<p>01_The solution: In the next notebook we present the theory behind Alternating Least Squares to solve recommendation systems, and the solutions for both small and large data sets.</p>
<ul>
<li>The main contribution of this part is a mathematical and algorithmic analysis to how the recommender works</li>
</ul>
</li>
<li>
<p>02_Extensions and future ideas: In the final notebook we create specific functions to improve the original receommendation system and propose future ideas.</p>
<ul>
<li>
<p>The main contributions of this part are the creation of a system that takes user info and outputs suggestions.</p>
</li>
<li>
<p>For first time users, create an algorithm that gives the top rated movies over all users.</p>
</li>
<li>
<p>Test the performance of the ALS algorithm for reccomendations based on movie's genres.</p>
</li>
</ul>
</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="auto">
<div class="output execute_result html_result" execution_count="1">
<iframe 
 src="https://en.wikipedia.org/wiki/Collaborative_filtering"
 width="95%" height="450"
 sandbox>
  <p>
    <a href="http://spark.apache.org/docs/latest/index.html">
      Fallback link for browsers that, unlikely, don't support frames
    </a>
  </p>
</iframe>
</div>
</div>
<div class="cell markdown">
<h2 id="data"><a class="header" href="#data">Data</a></h2>
<p>To test the scalability of our approach we use two different datasets, that contain users' ratings to movies:</p>
<ul>
<li>
<p>The original dataset stored in dbutils.fs.ls(&quot;/databricks-datasets/cs100/lab4/data-001/&quot;) and used in the original algorithm, consisting of 2999 users and 3615 movies, with a total of 487650 ratings.</p>
</li>
<li>
<p>A dataset from <a href="https://www.kaggle.com/netflix-inc/netflix-prize-data">kaggle</a>, used in a competition that Netflix held to improve recommendation systems. The dataset contains 480189 users and 17770 movies. Ratings are given on an integral scale from 1 to 5. The data is stored in dbutils.fs.ls(&quot;/FileStore/tables/Netflix&quot;).</p>
</li>
</ul>
</div>
<div class="cell markdown">
<h2 id="link-to-video"><a class="header" href="#link-to-video">Link to video</a></h2>
<p>https://kth.box.com/s/tyccs648wusbxcgd3nr0s24gwo5lmc1x</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="the-alternating-least-squares-method-als"><a class="header" href="#the-alternating-least-squares-method-als"><a href="contents/student-project-18_group-ProjectRL/">The Alternating Least Squares method (ALS)</a></a></h1>
</div>
<div class="cell markdown">
<h2 id="the-als-algorithm"><a class="header" href="#the-als-algorithm">The ALS algorithm</a></h2>
<p>The ALS algorithm was proposed in 2008 by <em>F. Zhang, E.Shang, Y. Xu and X. Wu</em> in a paper titled : <strong>Large-scale Parallel Colaborative Filtering for the Netflix Prize</strong> <a href="https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32">(paper)</a>. We will briefly describe the main ideas behind the ALS algorithm.</p>
<h4 id="what-are-we-learning-"><a class="header" href="#what-are-we-learning-">What are we learning ?</a></h4>
<p>In order to finding the missing values of the rating matrix R, the authors of the ALS algorithm considered approximating this matrix by a product of two tall matrices U and M of low rank. In other words, the goal is to find a low rank approximation of the ratings matrix R:</p>
<p>\[
R \approx U M^\top = \begin{bmatrix} u_1 &amp; \dots &amp; u_N \end{bmatrix}^\top  \begin{bmatrix}
m_1 &amp; \dots &amp; m_M \end{bmatrix} \qquad \text{where} \qquad U \in \mathbb{R}^{N \times K}, M \in \mathbb{R}^{M \times K}
\]</p>
<p>Intuitively we think of U (resp. M) as a matrix of users' features (resp. movies features) and we may rewrite this approximation entrywise as</p>
<p>\[
\forall i,j \qquad r_{i,j} \approx u_i^\top m_j. 
\]</p>
<h4 id="the-loss-function"><a class="header" href="#the-loss-function">The loss function</a></h4>
<p>If all entries of the rating matrix R were known, one may use an SVD decomposition to reconstruct U and M. However, not all ratings are known therefore one has to learn the matrices U and M. The authors of the paper proposed to minimize the following loss which corresponds to the sum of squares errors with a Thikonov rigularization that weighs the users matrix U (resp. the movies matrix M) using the Gamma<em>U (resp. Gamma</em>M)</p>
<p>\[
\mathcal{L}<em>{U,M}^{wheighted}(R) = \sum</em>{(i,j)\in S}
(r_{i,j} - u_i^\top m_j)^2 + \lambda \Vert M \Gamma_m \Vert^2 + \lambda \Vert U \Gamma_u \Vert^2 
\]</p>
<p>where S corresponds to the set of known ratings, \lambda is a regularaziation parameter. In fact this loss corresponds to the Alternating Least Squares with Weigted Regularization (ALS-WR). We will be using a variant of that algorithm a.k.a. the ALS algorithm which corresponds to minimizing the following slighltly similar loss without wheighing:</p>
<p>\[
\mathcal{L}<em>{U,M}(R) = \sum</em>{(i,j)\in S}
(r_{i,j} - u_i^\top m_j)^2 + \lambda \Vert M \Vert^2 + \lambda \Vert U \Vert^2 
\]</p>
<p>and the goal of the algorithm will be find a condidate (U,M) that</p>
<p>\[
\min_{U,M} \mathcal{L}_{U,M}(R)
\]</p>
<h4 id="the-als-algorithm-1"><a class="header" href="#the-als-algorithm-1">The ALS algorithm</a></h4>
<p>The authors approach to solve the aforementioned minimization problem as follows: - <strong>Step 1.</strong> Initialize matrix M, by assigning the average rating for that movie as the first row and small random numbers for the remaining entries. - <strong>Step 2.</strong> Fix M, Solve for U by minimizing the aformentioned loss. - <strong>Step 3.</strong> Fix U, solve for M by minimizing the aformentioned loss similarly. - <strong>Step 4.</strong> Repeat Steps 2 and 3 until a stopping criterion is satisfied.</p>
<p>Note that when one of the matrices is fixed, say M, the loss becomes quadratic in U and the solution corresponds to that of the least squares.</p>
<h4 id="key-parameters-of-the-algorithm"><a class="header" href="#key-parameters-of-the-algorithm">Key parameters of the algorithm</a></h4>
<p>The key parameters of the lagorithm are the <strong>rank K</strong>, the <strong>regularization parameter lambda</strong>, and the <strong>number of iterations</strong> befor stopping the algorithm. Indeed, since we don not have full knowledge of the matrix R, we do not know its rank. To find the best rank we will use cross-validation and dedicate part of the data to that. There is no straight way to choosing the regularization parameter, we will base our choice on reported values that work for the considered datasets. As for the number of iterations, we will proceed similarly.</p>
<h4 id="practically-speaking"><a class="header" href="#practically-speaking">Practically speaking</a></h4>
<p>We will use the following mllib library in scala wich contain classes dedicated to recommendation systems (See <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS</a>). More specifically, it contains the ALS class which allows for using the ALS algorithm as described earlier.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
</code></pre>
</div>
</div>
<div class="cell markdown">
<h1 id="on-a-small-dataset"><a class="header" href="#on-a-small-dataset"><a href="contents/student-project-18_group-ProjectRL/">On a small dataset</a></a></h1>
<p>This part of the notebook is borrowed from the notebook on the ALS we had in the course.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(dbutils.fs.ls(&quot;/databricks-datasets/cs100/lab4/data-001/&quot;)) // The data is already here
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/databricks-datasets/cs100/lab4/data-001/movies.dat</td>
<td>movies.dat</td>
<td>171308.0</td>
</tr>
<tr class="even">
<td>dbfs:/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz</td>
<td>ratings.dat.gz</td>
<td>2837683.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<h3 id="loading-the-data"><a class="header" href="#loading-the-data">Loading the data</a></h3>
<p>We read in each of the files and create an RDD consisting of parsed lines. Each line in the ratings dataset (<code>ratings.dat.gz</code>) is formatted as: <code>UserID::MovieID::Rating::Timestamp</code> Each line in the movies (<code>movies.dat</code>) dataset is formatted as: <code>MovieID::Title::Genres</code> The <code>Genres</code> field has the format <code>Genres1|Genres2|Genres3|...</code> The format of these files is uniform and simple, so we can use <code>split()</code>.</p>
<p>Parsing the two files yields two RDDs</p>
<ul>
<li>For each line in the ratings dataset, we create a tuple of (UserID, MovieID, Rating). We drop the timestamp because we do not need it for this exercise.</li>
<li>For each line in the movies dataset, we create a tuple of (MovieID, Title). We drop the Genres because we do not need them for this exercise.</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// take a peek at what's in the rating file
sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt; line.split(&quot;::&quot;) }.take(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res33: Array[Array[String]] = Array(Array(1, 1193, 5, 978300760), Array(1, 661, 3, 978302109), Array(1, 914, 3, 978301968), Array(1, 3408, 4, 978300275), Array(1, 2355, 5, 978824291))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val timedRatingsRDD = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (timestamp % 10, Rating(userId, movieId, rating))
      (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))
    }

timedRatingsRDD.take(10).map(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>(0,Rating(1,1193,5.0))
(9,Rating(1,661,3.0))
(8,Rating(1,914,3.0))
(5,Rating(1,3408,4.0))
(1,Rating(1,2355,5.0))
(8,Rating(1,1197,3.0))
(9,Rating(1,1287,5.0))
(9,Rating(1,2804,5.0))
(8,Rating(1,594,4.0))
(8,Rating(1,919,4.0))
timedRatingsRDD: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.recommendation.Rating)] = MapPartitionsRDD[9561] at map at command-3389902380791711:1
res34: Array[Unit] = Array((), (), (), (), (), (), (), (), (), ())
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val ratingsRDD = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: Rating(userId, movieId, rating)
      Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
    }

ratingsRDD.take(10).map(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Rating(1,1193,5.0)
Rating(1,661,3.0)
Rating(1,914,3.0)
Rating(1,3408,4.0)
Rating(1,2355,5.0)
Rating(1,1197,3.0)
Rating(1,1287,5.0)
Rating(1,2804,5.0)
Rating(1,594,4.0)
Rating(1,919,4.0)
ratingsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[9564] at map at command-3389902380791714:1
res35: Array[Unit] = Array((), (), (), (), (), (), (), (), (), ())
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val movies = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/movies.dat&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (movieId, movieName)
      (fields(0).toInt, fields(1))
    }.collect.toMap
</code></pre>
</div>
<div class="cell markdown">
<p>Let's make a data frame to visually explore the data next.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt; line.split(&quot;::&quot;) }.take(5)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>res36: Array[Array[String]] = Array(Array(1, 1193, 5, 978300760), Array(1, 661, 3, 978302109), Array(1, 914, 3, 978301968), Array(1, 3408, 4, 978300275), Array(1, 2355, 5, 978824291))
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val timedRatingsDF = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (timestamp % 10, Rating(userId, movieId, rating))
      (fields(3).toLong, fields(0).toInt, fields(1).toInt, fields(2).toDouble)
    }.toDF(&quot;timestamp&quot;, &quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;)


display(timedRatingsDF)
</code></pre>
</div>
<div class="cell markdown">
<p>Here we simply check the size of the datasets we are using</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val numRatings = ratingsRDD.count
val numUsers = ratingsRDD.map(_.user).distinct.count
val numMovies = ratingsRDD.map(_.product).distinct.count

println(&quot;Got &quot; + numRatings + &quot; ratings from &quot;
        + numUsers + &quot; users on &quot; + numMovies + &quot; movies.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Got 487650 ratings from 2999 users on 3615 movies.
numRatings: Long = 487650
numUsers: Long = 2999
numMovies: Long = 3615
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Now that we have the dataset we need, let's make a recommender system.</p>
<p><strong>Creating a Training Set, test Set and Validation Set</strong></p>
<p>Before we jump into using machine learning, we need to break up the <code>ratingsRDD</code> dataset into three pieces:</p>
<ul>
<li>A training set (RDD), which we will use to train models</li>
<li>A validation set (RDD), which we will use to choose the best model</li>
<li>A test set (RDD), which we will use for our experiments</li>
</ul>
<p>To randomly split the dataset into the multiple groups, we can use the <code>randomSplit()</code> transformation. <code>randomSplit()</code> takes a set of splits and seed and returns multiple RDDs.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val Array(trainingRDD, validationRDD, testRDD) = ratingsRDD.randomSplit(Array(0.60, 0.20, 0.20), 0L)
// let's find the exact sizes we have next
println(&quot; training data size = &quot; + trainingRDD.count() +
        &quot;, validation data size = &quot; + validationRDD.count() +
        &quot;, test data size = &quot; + testRDD.count() + &quot;.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code> training data size = 292318, validation data size = 97175, test data size = 98157.
trainingRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[9584] at randomSplit at command-3389902380791722:1
validationRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[9585] at randomSplit at command-3389902380791722:1
testRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[9586] at randomSplit at command-3389902380791722:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>After splitting the dataset, your training set has about 293,000 entries and the validation and test sets each have about 97,000 entries (the exact number of entries in each dataset varies slightly due to the random nature of the <code>randomSplit()</code> transformation.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// let's find the exact sizes we have next
println(&quot; training data size = &quot; + trainingRDD.count() +
        &quot;, validation data size = &quot; + validationRDD.count() +
        &quot;, test data size = &quot; + testRDD.count() + &quot;.&quot;)
                                                                                                                                          
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code> training data size = 292318, validation data size = 97175, test data size = 98157.
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="training-the-recommender-system"><a class="header" href="#training-the-recommender-system">Training the recommender system</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Build the recommendation model using ALS by fitting to the validation data
// just trying three different hyper-parameter (rank) values to optimise over
val ranks = List(4, 8, 12); 
var rank=0;
for ( rank &lt;- ranks ){
  val numIterations = 10
  val regularizationParameter = 0.01
  val model = ALS.train(trainingRDD, rank, numIterations, regularizationParameter)

  // Evaluate the model on test data
  val usersProductsValidate = validationRDD.map { case Rating(user, product, rate) =&gt;
                                              (user, product)
  }

  // get the predictions on test data
  val predictions = model.predict(usersProductsValidate)
                         .map { case Rating(user, product, rate)
                                     =&gt; ((user, product), rate)
    }

  // find the actual ratings and join with predictions
  val ratesAndPreds = validationRDD.map { case Rating(user, product, rate) 
                                     =&gt; ((user, product), rate)
                                   }.join(predictions)
  

  val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
    val err = (r1 - r2)
    err * err
  }.mean()
  
  println(&quot;rank and Mean Squared Error = &quot; +  rank + &quot; and &quot; + MSE)
} // end of loop over ranks
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rank and Mean Squared Error = 4 and 0.8479974514693542
rank and Mean Squared Error = 8 and 0.9300503484148622
rank and Mean Squared Error = 12 and 1.02609274473932
ranks: List[Int] = List(4, 8, 12)
rank: Int = 0
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Here we have the best model</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">  val rank = 4
  val numIterations = 10
  val regularizationParameter = 0.01
  val model = ALS.train(trainingRDD, rank, numIterations, regularizationParameter)

  // Evaluate the model on test data
  val usersProductsTest = testRDD.map { case Rating(user, product, rate) =&gt;
                                              (user, product)
  }

  // get the predictions on test data
  val predictions = model.predict(usersProductsTest)
                         .map { case Rating(user, product, rate)
                                     =&gt; ((user, product), rate)
    }

  // find the actual ratings and join with predictions
  val ratesAndPreds = testRDD.map { case Rating(user, product, rate) 
                                     =&gt; ((user, product), rate)
                                   }.join(predictions)

  val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
    val err = (r1 - r2)
    err * err
  }.mean()
  
  println(&quot;rank and Mean Squared Error for test data = &quot; +  rank + &quot; and &quot; + MSE)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rank and Mean Squared Error for test data = 4 and 0.8339905882351633
rank: Int = 4
numIterations: Int = 10
regularizationParameter: Double = 0.01
model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@4861c837
usersProductsTest: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[10463] at map at command-3389902380791728:7
predictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[10472] at map at command-3389902380791728:13
ratesAndPreds: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[10476] at join at command-3389902380791728:20
MSE: Double = 0.8339905882351633
</code></pre>
</div>
</div>
<div class="cell markdown">
<h1 id="on-a-large-dataset---netflix-dataset"><a class="header" href="#on-a-large-dataset---netflix-dataset"><a href="contents/student-project-18_group-ProjectRL/">On a large dataset - Netflix dataset</a></a></h1>
<h2 id="loading-the-data-1"><a class="header" href="#loading-the-data-1">Loading the data</a></h2>
<p>Netflix held a competition to improve recommendation systems. The dataset can be found in <a href="https://www.kaggle.com/netflix-inc/netflix-prize-data">kaggle</a>. Briefly speaking, the dataset contains users' ratings to movies, with 480189 users and 17770 movies. Ratings are given on an integral scale from 1 to 5. The first step is to download the data and store it in databricks. Originally, the dataset is plit into four files each with the following format:</p>
<pre><code>MovieID:
UserID, rating, date
.
.
.
MovieID: 
UserID, rating, date
.
.
.
</code></pre>
<p>We process these files so that each line has the format <code>MovieID, UserID, rating, date</code></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Path where the data is stored 
display(dbutils.fs.ls(&quot;/FileStore/tables/Netflix&quot;)) 
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/FileStore/tables/Netflix/combined_data_1_tar.xz</td>
<td>combined_data_1_tar.xz</td>
<td>1.19273784e8</td>
</tr>
<tr class="even">
<td>dbfs:/FileStore/tables/Netflix/combined_data_2_tar.xz</td>
<td>combined_data_2_tar.xz</td>
<td>1.33487548e8</td>
</tr>
<tr class="odd">
<td>dbfs:/FileStore/tables/Netflix/combined_data_3_tar.xz</td>
<td>combined_data_3_tar.xz</td>
<td>1.11976904e8</td>
</tr>
<tr class="even">
<td>dbfs:/FileStore/tables/Netflix/combined_data_4_tar.xz</td>
<td>combined_data_4_tar.xz</td>
<td>1.32669964e8</td>
</tr>
<tr class="odd">
<td>dbfs:/FileStore/tables/Netflix/formatted_combined_data_1_txt.gz</td>
<td>formatted_combined_data_1_txt.gz</td>
<td>1.66682858e8</td>
</tr>
<tr class="even">
<td>dbfs:/FileStore/tables/Netflix/formatted_combined_data_2_txt.gz</td>
<td>formatted_combined_data_2_txt.gz</td>
<td>1.87032103e8</td>
</tr>
<tr class="odd">
<td>dbfs:/FileStore/tables/Netflix/formatted_combined_data_3_txt.gz</td>
<td>formatted_combined_data_3_txt.gz</td>
<td>1.56042358e8</td>
</tr>
<tr class="even">
<td>dbfs:/FileStore/tables/Netflix/formatted_combined_data_4_txt.gz</td>
<td>formatted_combined_data_4_txt.gz</td>
<td>1.85177843e8</td>
</tr>
<tr class="odd">
<td>dbfs:/FileStore/tables/Netflix/movie_titles.csv</td>
<td>movie_titles.csv</td>
<td>577547.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell markdown">
<p>Let us load first the movie titles.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a Movie class 
case class Movie(movieID: Int, year: Int, tilte: String)

// Load the movie titles in an RDD
val moviesTitlesRDD: RDD[Movie] = sc.textFile(&quot;/FileStore/tables/Netflix/movie_titles.csv&quot;).map { line =&gt;
      val fields = line.split(&quot;,&quot;)
      // format: Rating(movieId, year, title)
      Movie(fields(0).toInt, fields(1).toInt, fields(2))
    }

// Print the titles of the first 3 movies 
moviesTitlesRDD.take(5).foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Movie(1,2003,Dinosaur Planet)
Movie(2,2004,Isle of Man TT 2004 Review)
Movie(3,1997,Character)
Movie(4,1994,Paula Abdul's Get Up &amp; Dance)
Movie(5,2004,The Rise and Fall of ECW)
defined class Movie
moviesTitlesRDD: org.apache.spark.rdd.RDD[Movie] = MapPartitionsRDD[129] at map at command-3389902380789882:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val RatingsRDD_1 = sc.textFile(&quot;/FileStore/tables/Netflix/formatted_combined_data_1_txt.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;,&quot;)
      // format: Rating(userId, movieId, rating))
      Rating(fields(1).toInt, fields(0).toInt, fields(2).toDouble)
    }

val RatingsRDD_2 = sc.textFile(&quot;/FileStore/tables/Netflix/formatted_combined_data_2_txt.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;,&quot;)
      // format: Rating(userId, movieId, rating))
      Rating(fields(1).toInt, fields(0).toInt, fields(2).toDouble)
    }

val RatingsRDD_3 = sc.textFile(&quot;/FileStore/tables/Netflix/formatted_combined_data_3_txt.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;,&quot;)
      // format: Rating(userId, movieId, rating))
      Rating(fields(1).toInt, fields(0).toInt, fields(2).toDouble)
    }

val RatingsRDD_4 = sc.textFile(&quot;/FileStore/tables/Netflix/formatted_combined_data_4_txt.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;,&quot;)
      // format: Rating(userId, movieId, rating))
      Rating(fields(1).toInt, fields(0).toInt, fields(2).toDouble)
    }


RatingsRDD_4.take(5).foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Rating(2385003,13368,4.0)
Rating(659432,13368,3.0)
Rating(751812,13368,2.0)
Rating(2625420,13368,2.0)
Rating(1650301,13368,1.0)
RatingsRDD_1: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[258] at map at command-3389902380789875:2
RatingsRDD_2: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[261] at map at command-3389902380789875:8
RatingsRDD_3: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[264] at map at command-3389902380789875:14
RatingsRDD_4: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[267] at map at command-3389902380789875:20
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Concatenating the ratings RDDs (could not find a nice way of doing this)
val r1 = RatingsRDD_1.union(RatingsRDD_2) 
val r2 = r1.union(RatingsRDD_3)
val RatingsRDD = r2.union(RatingsRDD_4)
RatingsRDD.take(5).foreach(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>r1: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = UnionRDD[278] at union at command-3389902380791426:2
r2: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = UnionRDD[279] at union at command-3389902380791426:3
RatingsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = UnionRDD[280] at union at command-3389902380791426:4
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Let us put our dataset in a dataframe to visulaize it more nicely</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val RatingsDF = RatingsRDD.toDF
display(RatingsDF)
</code></pre>
</div>
<div class="cell markdown">
<h2 id="training-the-movie-recommender-system"><a class="header" href="#training-the-movie-recommender-system">Training the movie recommender system</a></h2>
<p>In the training process we will start by splitting the dataset into - a training set (60%) - a validation set (20%) - a test set (20%)</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Splitting the dataset 
val Array(trainingRDD, validationRDD, testRDD) = RatingsRDD.randomSplit(Array(0.60, 0.20, 0.20), 0L)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code> training data size = 60288922, validation data size = 20097527, test data size = 20094058.
trainingRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[8350] at randomSplit at command-3389902380791433:1
validationRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[8351] at randomSplit at command-3389902380791433:1
testRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[8352] at randomSplit at command-3389902380791433:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>After splitting the dataset, your training set has about 60,288,922 entries and the validation and test sets each have about 20,097,527 entries (the exact number of entries in each dataset varies slightly due to the random nature of the <code>randomSplit()</code> transformation.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// let's find the exact sizes we have next
println(&quot; training data size = &quot; + trainingRDD.count() +
        &quot;, validation data size = &quot; + validationRDD.count() +
        &quot;, test data size = &quot; + testRDD.count() + &quot;.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code> training data size = 60288922, validation data size = 20097527, test data size = 20094058.
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Build the recommendation model using ALS by fitting to the validation data
// just trying three different hyper-parameter (rank) values to optimise over
val ranks = List(50, 100, 150, 300, 400, 500); 
var rank=0;
for ( rank &lt;- ranks ){
  val numIterations = 12
  val regularizationParameter = 0.05
  val model = ALS.train(trainingRDD, rank, numIterations, regularizationParameter)

  // Evaluate the model on test data
  val usersProductsValidate = validationRDD.map { case Rating(user, product, rate) =&gt;
                                              (user, product)
  }

  // get the predictions on test data
  val predictions = model.predict(usersProductsValidate)
                         .map { case Rating(user, product, rate)
                                     =&gt; ((user, product), rate)
    }

  // find the actual ratings and join with predictions
  val ratesAndPreds = validationRDD.map { case Rating(user, product, rate) 
                                     =&gt; ((user, product), rate)
                                   }.join(predictions)
  

  val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
    val err = (r1 - r2)
    err * err
  }.mean()
  
  println(&quot;rank and Mean Squared Error = &quot; +  rank + &quot; and &quot; + MSE)
} // end of loop over ranks
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rank and Mean Squared Error = 50 and 0.7060806826621556
rank and Mean Squared Error = 100 and 0.7059490573655225
rank and Mean Squared Error = 150 and 0.7056407494686934
</code></pre>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="extensions"><a class="header" href="#extensions">Extensions</a></h1>
<p>In this notebook, we introduce multiple improvements to the original algorithm, using the small dataset. We: - First, improve the original algorithm by creating a system that takes user info and outputs suggestions, which is the typical final role of a recommendation system. - Then, we add the functionality that for first time user, we output the top rated movies over all users. - Furthemore, we improve the existing model by including the movie's genres to give better recommendations.</p>
</div>
<div class="cell markdown">
<h2 id="preliminaries"><a class="header" href="#preliminaries">Preliminaries</a></h2>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// import the relevant libraries for `mllib`

import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
import org.apache.spark.sql.expressions.UserDefinedFunction
import scala.collection.mutable.WrappedArray
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
import org.apache.spark.sql.expressions.UserDefinedFunction
import scala.collection.mutable.WrappedArray
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Get the small dataset and display information
display(dbutils.fs.ls(&quot;/databricks-datasets/cs100/lab4/data-001/&quot;)) // The data is already here
</code></pre>
<div class="output execute_result tabular_result" execution_count="1">
<table>
<thead>
<tr class="header">
<th>path</th>
<th>name</th>
<th>size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dbfs:/databricks-datasets/cs100/lab4/data-001/movies.dat</td>
<td>movies.dat</td>
<td>171308.0</td>
</tr>
<tr class="even">
<td>dbfs:/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz</td>
<td>ratings.dat.gz</td>
<td>2837683.0</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create the RDD containing the ratings

val ratingsRDD = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: Rating(userId, movieId, rating)
      Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
}
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>ratingsRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[21729] at map at command-3389902380791579:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// We take a look at the first 10 entries in the Ratings RDD
ratingsRDD.take(10).map(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Rating(1,1193,5.0)
Rating(1,661,3.0)
Rating(1,914,3.0)
Rating(1,3408,4.0)
Rating(1,2355,5.0)
Rating(1,1197,3.0)
Rating(1,1287,5.0)
Rating(1,2804,5.0)
Rating(1,594,4.0)
Rating(1,919,4.0)
res86: Array[Unit] = Array((), (), (), (), (), (), (), (), (), ())
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>A similar command is used to format the movies. For this first part the genre field is ignored. They will considered in the second part of this notebook.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val movies = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/movies.dat&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (movieId, movieName)
      (fields(0).toInt, fields(1))
    }.collect.toMap
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// check the size of the small dataset

val numRatings = ratingsRDD.count
val numUsers = ratingsRDD.map(_.user).distinct.count
val numMovies = ratingsRDD.map(_.product).distinct.count

println(&quot;Got &quot; + numRatings + &quot; ratings from &quot;
        + numUsers + &quot; users on &quot; + numMovies + &quot; movies.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Got 487650 ratings from 2999 users on 3615 movies.
numRatings: Long = 487650
numUsers: Long = 2999
numMovies: Long = 3615
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Creating a Training Set, test Set and Validation Set

val Array(trainingRDD, validationRDD, testRDD) = ratingsRDD.randomSplit(Array(0.60, 0.20, 0.20), 0L)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>trainingRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[21741] at randomSplit at command-3389902380791584:3
validationRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[21742] at randomSplit at command-3389902380791584:3
testRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[21743] at randomSplit at command-3389902380791584:3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// let's find the exact sizes we have next
println(&quot; training data size = &quot; + trainingRDD.count() +
        &quot;, validation data size = &quot; + validationRDD.count() +
        &quot;, test data size = &quot; + testRDD.count() + &quot;.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code> training data size = 292318, validation data size = 97175, test data size = 98157.
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="ratings-distribution"><a class="header" href="#ratings-distribution">Ratings distribution</a></h2>
<p>For curiosity, we start by plotting the histogram of the ratings present in this dataset</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create a DataFrame with the data
import org.apache.spark.sql.functions._

val ratingsDF = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (timestamp % 10, Rating(userId, movieId, rating))
      (fields(0).toInt, fields(1).toInt, fields(2).toDouble)
    }.toDF(&quot;userID&quot;, &quot;movieID&quot;, &quot;rating&quot;)
display(ratingsDF)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val history = ratingsDF.groupBy(&quot;rating&quot;).count().orderBy(asc(&quot;rating&quot;))
history.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+------+
|rating| count|
+------+------+
|   1.0| 27472|
|   2.0| 53838|
|   3.0|127216|
|   4.0|170579|
|   5.0|108545|
+------+------+

history: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [rating: double, count: bigint]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">display(history)
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/18_02_1.JPG?raw=true" alt="" /></p>
</div>
<div class="cell markdown">
<h2 id="create-a-system-that-takes-user-info-and-outputs-suggestions"><a class="header" href="#create-a-system-that-takes-user-info-and-outputs-suggestions">Create a system that takes user info and outputs suggestions.</a></h2>
<p>user info = ((movieID,rating),(movieID,rating)). It is basically an (incomplete) line in the ratings matrix.</p>
<ul>
<li>Choose an user</li>
<li>Run the model and fill the columns - predict the ratings for the movies</li>
<li>Output the ones with the best predicted score</li>
</ul>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Train the model as usually
  val rank = 4
  val numIterations = 10
  val regularizationParameter = 0.01
  val model = ALS.train(trainingRDD, rank, numIterations, regularizationParameter)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rank: Int = 4
numIterations: Int = 10
regularizationParameter: Double = 0.01
model: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@570cf4b3
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Choose any random user,which is going to be our test user
val newUserID = 1000

// Create a list with the MovieIds we want to predict its rating to
val newUser = Array(Rating(newUserID, 1, 0),Rating(newUserID, 2, 0),Rating(newUserID.toInt, 3, 0),Rating(newUserID.toInt, 4, 0),Rating(newUserID.toInt, 5, 0))
newUser.map(println)

// Convert it to an RDD
val newTest = sc.parallelize(newUser)
newTest.map(println)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Rating(1000,1,0.0)
Rating(1000,2,0.0)
Rating(1000,3,0.0)
Rating(1000,4,0.0)
Rating(1000,5,0.0)
newUserID: Int = 1000
newUser: Array[org.apache.spark.mllib.recommendation.Rating] = Array(Rating(1000,1,0.0), Rating(1000,2,0.0), Rating(1000,3,0.0), Rating(1000,4,0.0), Rating(1000,5,0.0))
newTest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = ParallelCollectionRDD[21968] at parallelize at command-3389902380791591:9
res94: org.apache.spark.rdd.RDD[Unit] = MapPartitionsRDD[21969] at map at command-3389902380791591:10
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">  // Evaluate the model on this test user
  val usersProductsTest = newTest.map { case Rating(user, product, rate) =&gt;
                                              (user, product)
  }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>usersProductsTest: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[21970] at map at command-3389902380791592:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">  // get the predictions for this test user
  val predictions = model.predict(usersProductsTest)
                         .map { case Rating(user, product, rate)
                                     =&gt; ((user, product), rate)
    }

  val ratesAndPreds = newTest.map { case Rating(user, product, rate) 
                                     =&gt; ((user, product), rate)
                                   }.join(predictions)

</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>predictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[21979] at map at command-3389902380791593:3
ratesAndPreds: org.apache.spark.rdd.RDD[((Int, Int), (Double, Double))] = MapPartitionsRDD[21983] at join at command-3389902380791593:9
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert the RDD with the predictions to a DataFrame
val preds2 = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt; (user,product,r2) }

var predsDF = preds2.toDF(&quot;userID&quot;,&quot;movieID&quot;,&quot;pred&quot;)


predsDF.orderBy(asc(&quot;movieID&quot;))show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+-------+------------------+
|userID|movieID|              pred|
+------+-------+------------------+
|  1000|      1|4.3134486083287396|
|  1000|      2| 3.561695001470941|
|  1000|      3| 3.251747295342854|
|  1000|      4|2.9727526635707116|
|  1000|      5|3.1890542732727987|
+------+-------+------------------+

preds2: org.apache.spark.rdd.RDD[(Int, Int, Double)] = MapPartitionsRDD[21984] at map at command-3389902380791594:2
predsDF: org.apache.spark.sql.DataFrame = [userID: int, movieID: int ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Order the movies according to the predictions
val orderedPreds = predsDF.orderBy(desc(&quot;pred&quot;))
orderedPreds.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+------+-------+------------------+
|userID|movieID|              pred|
+------+-------+------------------+
|  1000|      1|4.3134486083287396|
|  1000|      2| 3.561695001470941|
|  1000|      3| 3.251747295342854|
|  1000|      5|3.1890542732727987|
|  1000|      4|2.9727526635707116|
+------+-------+------------------+

orderedPreds: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userID: int, movieID: int ... 1 more field]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Return the ID of the highest recommended one

val t = orderedPreds.select(&quot;movieID&quot;).collect().map(_(0)).toList.take(1)
println(&quot;The movie highest recommended for this user is:&quot;)
println(movies(t(0).asInstanceOf[Int]))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>The movie highest recommended for this user is:
Toy Story (1995)
t: List[Any] = List(1)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="for-first-time-users-the-program-gives-the-top-rated-movies-over-all-users"><a class="header" href="#for-first-time-users-the-program-gives-the-top-rated-movies-over-all-users">For first time users, the program gives the top rated movies over all users.</a></h2>
<p>If newUser: - Check the ratings matrix - Compute the average rating of each column (of each movie) - Return the columns with the highest</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Note: This is only for the ones they said. Doesnt include the ones computed by our model...
import org.apache.spark.sql.functions._

val newUserID = 4000

// Compute the average of each movie
val averageRates = ratingsDF.groupBy(&quot;movieID&quot;).avg(&quot;rating&quot;)
averageRates.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+-------+------------------+
|movieID|       avg(rating)|
+-------+------------------+
|   1580|3.7045454545454546|
|   2366|           3.71875|
|   1088|  3.29595015576324|
|   1959|3.6577181208053693|
|   3175|3.8145454545454545|
|   1645| 3.367021276595745|
|    496|3.3846153846153846|
|   2142|2.8256880733944953|
|   1591|2.5783132530120483|
|   2122|2.3434343434343434|
|    833| 2.130434782608696|
|    463|2.7222222222222223|
|    471| 3.665492957746479|
|   1342|2.8188976377952755|
|    148| 2.857142857142857|
|   3918| 2.806896551724138|
|   3794|               3.4|
|   1238|3.9526627218934913|
|   2866|3.7386363636363638|
|   3749|               4.0|
+-------+------------------+
only showing top 20 rows

import org.apache.spark.sql.functions._
newUserID: Int = 4000
averageRates: org.apache.spark.sql.DataFrame = [movieID: int, avg(rating): double]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Order the movies by top ratings
val orderedRates = averageRates.orderBy(desc(&quot;avg(rating)&quot;)).withColumnRenamed(&quot;avg(rating)&quot;,&quot;avg_rate&quot;)
orderedRates.show()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>+-------+-----------------+
|movieID|         avg_rate|
+-------+-----------------+
|    854|              5.0|
|    853|              5.0|
|    787|              5.0|
|   1830|              5.0|
|   3881|              5.0|
|    557|              5.0|
|   3280|              5.0|
|    578|              5.0|
|   2444|              5.0|
|   3636|              5.0|
|   3443|              5.0|
|   3800|              5.0|
|    989|              5.0|
|   1002|4.666666666666667|
|   3232|4.666666666666667|
|   2839|4.666666666666667|
|   3245|4.666666666666667|
|   2905|4.609756097560975|
|   1743|              4.6|
|   2019|4.586330935251799|
+-------+-----------------+
only showing top 20 rows

orderedRates: org.apache.spark.sql.DataFrame = [movieID: int, avg_rate: double]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Return the top 5 movies with highest ratings over all users

val topMovies = orderedRates.take(5)
//println(topMovies)
//topMovies.foreach(t =&gt; println(t(0)))
val moviesList = orderedRates.select(&quot;movieID&quot;).collect().map(_(0)).toList.take(5)
//println(moviesList)

println(&quot;The movies recommended for a new user based on the overall rating are:&quot;)
for (t &lt;-  moviesList )
    println(movies(t.asInstanceOf[Int]))
 // println(movies(t))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>The movies recommended for a new user based on the overall rating are:
Dingo (1992)
Gate of Heavenly Peace, The (1995)
Hour of the Pig, The (1993)
Those Who Love Me Can Take the Train (Ceux qui m'aiment prendront le train) (1998)
Schlafes Bruder (Brother of Sleep) (1995)
topMovies: Array[org.apache.spark.sql.Row] = Array([989,5.0], [787,5.0], [853,5.0], [578,5.0], [3636,5.0])
moviesList: List[Any] = List(853, 787, 578, 3636, 989)
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// In alternative, return the top movies with rating of 5 over all users

val topMovies5 = orderedRates.where(&quot;avg_rate == 5&quot;).select(&quot;movieID&quot;).collect().map(_(0)).toList

println(&quot;The movies recommended for a new user based on the overall rating are:&quot;)
for (t &lt;-  topMovies5 )
    println(movies(t.asInstanceOf[Int]))
 // println(movies(t))

</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>The movies recommended for a new user based on the overall rating are:
Dingo (1992)
Gate of Heavenly Peace, The (1995)
Hour of the Pig, The (1993)
Those Who Love Me Can Take the Train (Ceux qui m'aiment prendront le train) (1998)
Schlafes Bruder (Brother of Sleep) (1995)
Ballad of Narayama, The (Narayama Bushiko) (1958)
Baby, The (1973)
24 7: Twenty Four Seven (1997)
Born American (1986)
Criminal Lovers (Les Amants Criminels) (1999)
Follow the Bitch (1998)
Bittersweet Motel (2000)
Mamma Roma (1962)
topMovies5: List[Any] = List(853, 787, 578, 3636, 989, 854, 3280, 2444, 3443, 3800, 1830, 3881, 557)
</code></pre>
</div>
</div>
<div class="cell markdown">
<h2 id="genres-analysis"><a class="header" href="#genres-analysis">Genres analysis</a></h2>
<h6 id="we-investigate-whether-suggestion-based-on-genre-can-be-more-accurate-imagine-a-scenario-in-which-an-user-is-interested-in-watching-a-movie-of-a-particular-genre-say-an-animation-movie-given-this-information-can-we-suggest-a-better-film-with-respect-to-the-film-that-we-would-have-suggested-by-only-knowing-users-previous-ratings-on-such-movie"><a class="header" href="#we-investigate-whether-suggestion-based-on-genre-can-be-more-accurate-imagine-a-scenario-in-which-an-user-is-interested-in-watching-a-movie-of-a-particular-genre-say-an-animation-movie-given-this-information-can-we-suggest-a-better-film-with-respect-to-the-film-that-we-would-have-suggested-by-only-knowing-users-previous-ratings-on-such-movie">we investigate whether suggestion based on genre can be more accurate. Imagine a scenario in which an user is interested in watching a movie of a particular genre, say an Animation movie, given this information, can we suggest a better film with respect to the film that we would have suggested by only knowing user‚Äôs previous ratings on such movie?</a></h6>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Read the movies file as a dataframe and display it
val movies_df = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/movies.dat&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (movieId, movieName,genre)
      (fields(0).toInt, fields(1),fields(2).split(&quot;\\|&quot;))
    }.toDF(&quot;movieId&quot;, &quot;movieName&quot;, &quot;genre&quot;)

display(movies_df)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Select a GENRE, or a set of GENREs and filter the movies dataset according to this genre
val GENRE = &quot;Animation&quot;

def array_contains_any(s:Seq[String]): UserDefinedFunction = {
udf((c: WrappedArray[String]) =&gt;
  c.toList.intersect(s).nonEmpty)}

val b: Array[String] = Array(GENRE)
val genre_df = movies_df.where(array_contains_any(b)($&quot;genre&quot;))
display(genre_df)

val movie_ID_genres = genre_df.select(&quot;movieId&quot;).rdd.map(r =&gt; r(0)).collect()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// We now read and display the ratings dataframe (without the timestamp field) as a dataframe.
val RatingsDF = sc.textFile(&quot;/databricks-datasets/cs100/lab4/data-001/ratings.dat.gz&quot;).map { line =&gt;
      val fields = line.split(&quot;::&quot;)
      // format: (timestamp % 10, Rating(userId, movieId, rating))
      (fields(0).toInt, fields(1).toInt, fields(2).toDouble)
    }.toDF(&quot;userId&quot;, &quot;movieId&quot;, &quot;rating&quot;)
display(RatingsDF)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Based on the movies id obtained by the filtering on the movie dataset we filter the ratings df and we convert it to rdd format
val Ratings_genre_df = RatingsDF.filter($&quot;movieId&quot;.isin(movie_ID_genres:_*))
val genre_rdd = Ratings_genre_df.rdd
display(Ratings_genre_df)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Print some dataset statistics
val numRatings = genre_rdd.count
println(&quot;Got &quot; + numRatings + &quot; ratings&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>Got 22080 ratings
numRatings: Long = 22080
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Create train, test, and evaluation dataset and print some statistics 
val Array(temp_trainingRDD, temp_validationRDD, temp_testRDD) = genre_rdd.randomSplit(Array(0.60, 0.20, 0.20), 0L)

// let's find the exact sizes we have next
println(&quot;training data size = &quot; + temp_trainingRDD.count() +
        &quot;, validation data size = &quot; + temp_validationRDD.count() +
        &quot;, test data size = &quot; + temp_testRDD.count() + &quot;.&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>training data size = 13229, validation data size = 4411, test data size = 4440.
temp_trainingRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19929] at randomSplit at command-3389902380791621:2
temp_validationRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19930] at randomSplit at command-3389902380791621:2
temp_testRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[19931] at randomSplit at command-3389902380791621:2
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Map the rdds to the Rating type
val maptrainingRDD = temp_trainingRDD.map(x=&gt;Rating(x(0).asInstanceOf[Int], x(1).asInstanceOf[Int], x(2).asInstanceOf[Double]))
val mapvalidationRDD = temp_validationRDD.map(x=&gt;Rating(x(0).asInstanceOf[Int], x(1).asInstanceOf[Int], x(2).asInstanceOf[Double]))
val maptestRDD = temp_testRDD.map(x=&gt;Rating(x(0).asInstanceOf[Int], x(1).asInstanceOf[Int], x(2).asInstanceOf[Double]))
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>maptrainingRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[19932] at map at command-3389902380791624:2
mapvalidationRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[19933] at map at command-3389902380791624:3
maptestRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[19934] at map at command-3389902380791624:4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Build the recommendation model using ALS by fitting to the training data (with Hyperparameter tuning)
// trying different hyper-parameter (rank) values to optimise over
val ranks = List(2, 4, 8, 16, 32, 64, 128, 256); 
var rank=0;

for ( rank &lt;- ranks ){
  // using a fixed numIterations=10 and regularisation=0.01
  val numIterations = 10
  val regularizationParameter = 0.01
  val model = ALS.train(maptrainingRDD, rank, numIterations, regularizationParameter)

  // Evaluate the model on test data
  val usersProductsValidate = mapvalidationRDD.map { case Rating(user, product, rate) =&gt;
                                              (user, product)
  }

  // get the predictions on test data
  val predictions = model.predict(usersProductsValidate)
                         .map { case Rating(user, product, rate)
                                     =&gt; ((user, product), rate)
    }

  // find the actual ratings and join with predictions
  val ratesAndPreds = mapvalidationRDD.map { case Rating(user, product, rate) 
                                     =&gt; ((user, product), rate)
                                   }.join(predictions)
  

  val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =&gt;
    val err = (r1 - r2)
    err * err
  }.mean()
  println(&quot;rank and Mean Squared Error = &quot; +  rank + &quot; and &quot; + MSE)
} // end of loop over ranks
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rank and Mean Squared Error = 2 and 1.0624116959469154
rank and Mean Squared Error = 4 and 1.3393495403657538
rank and Mean Squared Error = 8 and 1.6916511125697133
rank and Mean Squared Error = 16 and 1.63542207107039
rank and Mean Squared Error = 32 and 1.311227268934932
rank and Mean Squared Error = 64 and 0.9461947532838
rank and Mean Squared Error = 128 and 0.8859420827613572
rank and Mean Squared Error = 256 and 0.8845268169033572
numIterations: Int = 10
regularisation: Double = 0.01
ranks: List[Int] = List(2, 4, 8, 16, 32, 64, 128, 256)
rank: Int = 0
</code></pre>
</div>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
