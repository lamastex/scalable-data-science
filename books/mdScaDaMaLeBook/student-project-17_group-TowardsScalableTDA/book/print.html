<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>sds-3.x/ScaDaMaLe</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="scroll-mdbook-outputs.css">

        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="contents/student-project-17_group-TowardsScalableTDA/00_introduction.html">00_introduction</a></li><li class="chapter-item expanded affix "><a href="contents/student-project-17_group-TowardsScalableTDA/01_methodology.html">01_methodology</a></li><li class="chapter-item expanded affix "><a href="contents/student-project-17_group-TowardsScalableTDA/02_gaussian_analysis.html">02_gaussian_analysis</a></li><li class="chapter-item expanded affix "><a href="contents/student-project-17_group-TowardsScalableTDA/03_robotics_dataset.html">03_robotics_dataset</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">sds-3.x/ScaDaMaLe</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="density-estimation-via-voronoi-diagrams-in-high-dimensions"><a class="header" href="#density-estimation-via-voronoi-diagrams-in-high-dimensions">Density Estimation via Voronoi Diagrams in High Dimensions</a></h1>
</div>
<div class="cell markdown">
<p>Robert Gieselmann and Vladislav Polianskii</p>
<p><a href="https://drive.google.com/file/d/14E_igECN6hDZieWNn9VVTepCo5mu-rzy/view?usp=sharing">Video of project presentation</a></p>
</div>
<div class="cell markdown">
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
</div>
<div class="cell markdown">
<p><strong>Density estimation</strong> is a wide sub-area of statistics, tasked with understanding an underlying probability distribution of a given set of points, sampled from an unknown distribution. It can be used as a way of data investigation, like determining the location of low- and high-density regions in data, clusters and outliers, as well as for visualization purposes.</p>
<p>A histogram can be considered as a simple density estimator. Other well-known methods include: - a k-nearest-neighbor density estimator, which describes the density <em>p()</em> at a point <em>x</em> as \[p(x) \cong \frac{1}{d_k(x)}\] where d_k(x) is the distance to the <em>k</em>th nearest neighbor of <em>x</em>; - a kernel density estimator, which requires a selection of a kernel probability distribution <em>K</em> and a bandwidth <em>h</em> and essentially places the distributions at the data points, giving the density estimation \[p(x) \cong \sum_i K(\frac{x - x_i}{h})\]</p>
<p>All of the mentioned methods are sensitive to parameter selection, such as choosing the right number of neighbors or a fitting bandwidth.</p>
</div>
<div class="cell markdown">
<p><strong>Voronoi diagrams</strong> are widely used in many areas, including computer science, and provide a natural cell decomposition of space based on the nearest-neighbor rule. For a given data point <em>x</em>, its corresponding cell contains all the points of the metric space, for which <em>x</em> is the closest point among all in the dataset.</p>
<p>An example of a 2D Voronoi diagram built over a set of points sampled from a normal distribution can be seen below in the methodology part.</p>
<p>One of the biggest drawbacks of Voronoi diagrams is their geometric complexity, which grows exponentially with dimensionality and essentially prevents their exact computation in dimensions above 6 for a reasonable number of points. In the worst case, the number of geometric elements of the diagram (such as Voronoi vertices, edges and polyhedra of different dimensions that arise on the cell boundaries) grows as</p>
<p>\[O(n^{\lceil{d/2}\rceil})\]</p>
</div>
<div class="cell markdown">
<p><strong>Our method.</strong> In this work, we use some intuition about the Voronoi diagrams to develop a new method of density estimation. In addition, we apply a methodology from our previous work which allows one to work with Voronoi diagrams in high dimensions without their explicit construction.</p>
</div>
<div class="cell markdown">
<h2 id="methodology"><a class="header" href="#methodology">Methodology</a></h2>
</div>
<div class="cell markdown">
<p><strong>Intuition:</strong> if we construct a Voronoi diagram over a set of points sampled from an unknown distribution then Voronoi cells in regions with higher density will be of a smaller <em>size</em>.</p>
<p>Consider the image below, which depicts a Voronoi diagram in a two-dimensional space built over points sampled from a Gaussian distribution. Voronoi cells in the center of the distribution appear naturally smaller in comparison with other cells, and the cell size increases when we move away from the center.</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/voronoi_gaussian.png?raw=true" alt="" /></p>
<p>This intuition follows, in a way, a one-nearest-neighbor density estimator: the distance <em>d</em> to the nearest neighbor is inversly proportional to the estimated density of the point, and at the same time, a ball of radius <em>d/2</em> centered at the query point always fits into (and touches the boundary of) the Voronoi cell.</p>
<p>On the discussed image, one of the cells is marked with a blue color. Assume that the point inside that cell is our query point, at which we want to understand the density, and all other points are the training (unlabeled) data that provides information about the density. Then, let us try to find a reasonable approximation of the density in a form of</p>
<p>\[p(x) = \frac{c}{size(Cell(x))}\]</p>
<p>where <em>c</em> is some constant, <em>Cell</em> denotes the Voronoi cell of <em>x</em>, and <em>size</em> is some measure of a cell.</p>
<p>Note: at any moment, the Voronoi diagram consists of only one query point and all dataset points.</p>
</div>
<div class="cell markdown">
<p><strong>Volume function</strong></p>
<p>Let us assume for a while that cell's geometry is known to us. What would be a natural way to describe the size of the cell?</p>
<p>Perhaps, one of the first ideas that comes to mind is to use the cell's <em>volume</em> as a size measure. Here we run into an issue of infinite cells, whose volume would also be infinite. Potentially, this could be resolved by computing a weighted volume with an integrable weight function that rapidly decays at infinity.</p>
<p>However, instead, we propose a way to describe the size via <em>volume functions</em>, inspired by how alpha-complexes are motivated and constructed in the area of topological data analysis, where we consider a set of balls of an increasing radius with intersection with voronoi cells:</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/alpha_1.png?raw=true" alt="" /> <img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/alpha_2.png?raw=true" alt="" /> <img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/alpha_3.png?raw=true" alt="" /></p>
<p>We define the volume function as follows:</p>
<p>\[\overline{Vol}_d(x)(r) = \frac{Vol_d(Cell(x) \cap B_r(x))}{Vol_d(B_r)}\]</p>
<p>Here, <em>r</em> is a positive radius, <em>Vol()</em> denotes the standard d-dimensional volume, and *B_r(x)* is a d-dimensional ball of radius <em>r</em> centered at <em>x</em>. The volume function of <em>x</em> returns a function that takes a radius <em>r</em> and returns a ratio of the volume of the intersection of the ball with the cell to the whole volume of the ball. Clearly, at the limit to zero, the ratio is equal to 1 (when the ball fully fits inside the cell), but starts to decrease as soon as parts of the ball start to leave the boundary.</p>
<p>Below are two images. On the left, a simple rectangular Voronoi cell with a point, generating it. On the right, a depiction of the volume function for this cell.</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/rect.png?raw=true" alt="" /> <img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/rect_vol.png?raw=true" alt="" /></p>
<p>If we go into higher dimensions, we will not be able to see the steps that the function makes anymore. Below is an example, which we approximated (with a method described below) on MNIST data (784-dimensional) some time ago of volume functions for different data points:</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/mnist_vol.png?raw=true" alt="" /></p>
<p>On the picture above, we can guess that, for example, the point with the light-blue volume curve is located in a lower-density region than other given points, based on the fact that its volume function is greater than other functions at every radius.</p>
<p>A couple of things to consider here. 1. If a cell is infinite, then its volume function will not tend to 0 at infinity. Instead, it will tend to the angular size of this infinity. 2. If one cell can be placed inside another cell, identifying their generator points and rotating arbitrarily, the first volume function will be below the second volume function.</p>
<p>The second bullet point provides an idea that maybe we want to integrate this volume functions and compare them: a function with a larger integral would denote a lower-density region. At the same time, the first bullet point tells us that the functions are not always integrable. Thus, in this project we do the following modifications: we do not consider the directions of the balls which end up in infinity. To be more precise, we replace *B_r* with its <em>sector</em> where the voronoi cell is finite, in the formula for the volume function. This helps to mitigate the integrability issues.</p>
<p>Before we go into details about the computational aspects, we need to mention another modification to the formula. Instead of computing the d-dimensional volumes of balls, we decided to compute the (d-1)-dimensional volumes of spheres (or, the surface area of the balls). This modification makes the computation much easier. For example, the approximations of the volume functions become piecewise-constant.</p>
<p>Therefore, the formula for the <em>size(x)</em> becomes:</p>
<p>\[size(x) = \int_0^{inf}{\overline{Vol}<em>{d-1}(x)(r) dr} = \int_0^{inf}{ \frac{Vol</em>{d-1}(Cell(x) \cap \hat{S}<em>r(x))}{Vol</em>{d-1}( \hat{S}_r )} dr}\]</p>
<p>where *S_r(x)* denotes a hypersphere of radius <em>r</em>, and a &quot;^&quot; denotes that we only consider sections of a sphere where the cell is finite.</p>
</div>
<div class="cell markdown">
<p><strong>Integral computation.</strong></p>
<p>We perform a Monte-Carlo sampling integration method to approximate the volume function, a motivation for which is described in detail in one of our earlier papers about Voronoi Boundary Classification (http://proceedings.mlr.press/v97/polianskii19a.html).</p>
<p>In short details, we sample random rays in uniform directions (equivalently, we sample points uniformly on the unit hypersphere), starting from the query point. For each ray, we record where it hits the boundary of the Voronoi cell. The length is computed by the following equation:</p>
<p>\[l(x, m) = \min_{i=1..N, \langle m, x - x_i \rangle &gt; 0} \frac{\lVert x - x_i \rVert^2}{2\langle m, x - x_i \rangle }\]</p>
<p>Here, <em>x</em> is the origin of the ray (the generator/query point), <em>m</em> is the directional unit vector, *x_i* are other data points. The &quot;infinite&quot; directions are excluded. The condition in the minimum signifies, that we are only interested in the positive length, i.e. we can't find an intersection behind the ray.</p>
<p>After casting <em>T</em> rays from a point, we can approximate the volume function as:</p>
<p>\[\overline{Vol}<em>{d-1}(x)(r) = \frac{1}{T}\sum</em>{t=1}^{T} \mathbb{1}\left[l(x, m_t) \ge r \right]\]</p>
<p>The integral of the function can be easily computed as a sum of all lengths:</p>
<p>\[size(x) = \frac{1}{T}\sum_{t=1}^{T} l(x, m_t)\]</p>
<p>And, our (unnormalized) density:</p>
<p>\[\tilde{p}(x) = \frac{T}{\sum_{t=1}^{T} l(x, m_t)}\]</p>
<p>Overall, the method's compexity with some optimizations is:</p>
<p>\[O(NMT + NMD + NTD + MTD)\]</p>
<p>where <em>N</em> is the number of train points, <em>M</em> is the number of query points, <em>T</em> is the number of rays from each point and <em>D</em> is data dimensionality.</p>
</div>
<div class="cell markdown">
<p><strong>Ranking loss.</strong></p>
<p>At the moment, we do not have any proofs that this indeed generates an unnormalized approximation for the density.</p>
<p>However, we are fairly certain (though also without a proof) that the approximation, when the dataset size tends to infinity, approximates the correct &quot;ranking&quot; of the estimates. Namely,</p>
<p>\[p(x_1) &lt; p(x_2) \Leftrightarrow \tilde{p}(x_1) &lt; \tilde{p}(x_2)\]</p>
<p>with probability 1 when data size is large enough. Here <em>p</em> is the real density used for point sampling, and <em>\tilde{p}</em> is the approximation.</p>
<p>This quality is meaningful in tasks when we need to sort points according to their density. For example, if we want to exclude noise (say, 5% of the all points with the lowest density), or use for density filtration in topological data analysis.</p>
<p>A measure that we use to estimate how well we approximate the correct density ranking works as following: 1. Sort available query points according to their true density. 2. Sort available query points according to the approximated density. 3. Find the number of inverses (swaps of two consecutive elements) required to obtain the first sequence of points from the second one.</p>
<p>The can easily be counted with a merge-sort algorithm in n log n time, but for simplicity and testing purposes (also because we use python for that) we do it in a simple quadratic time.</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="methodology-1"><a class="header" href="#methodology-1">Methodology</a></h1>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.rdd.RDD
import breeze.linalg._
import breeze.numerics._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.rdd.RDD
import breeze.linalg._
import breeze.numerics._
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Constants required for the method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Constants
val N = 250 // train size
val M = 250 // test size
val D = 2 // dimensionality
val T = 500 // number of rays
val one_vs_all = true

assert((!one_vs_all) || N == M)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>N: Int = 250
M: Int = 250
D: Int = 2
T: Int = 500
one_vs_all: Boolean = true
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Generate points from standard Gaussian distribution.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">val train_data = RandomRDDs.normalVectorRDD(sc, N, D).zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
val test_data = if(one_vs_all) train_data else RandomRDDs.normalVectorRDD(sc, M, D).zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>train_data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[10486] at map at command-1767923094595286:1
test_data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[10486] at map at command-1767923094595286:1
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Generate T random rays</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def get_uni_sphere() = {
  var u = RandomRDDs.normalVectorRDD(sc, T, D)
  u = new Normalizer().transform(u)
  var t = u.zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
  t
}
  
val rays = get_uni_sphere()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>get_uni_sphere: ()org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])]
rays: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[10490] at map at command-685894176423986:4
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Compute optimizations: all squared distances and all dot products of points with directions vectors.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_dst_sq() = { // (N, M)
  // dst[n, m] = |x_n - x'_m|^2
  val dst = train_data.cartesian(test_data).map { case ((n, train_vec), (m, test_vec)) =&gt; ((n, m), sum(((train_vec - test_vec) *:* (train_vec - test_vec)) ^:^ 2.0) ) }
  dst
}

def compute_pu(data: RDD[(Long, DenseVector[Double])]) = { // (data.N, T)
  // pu[n, t] = &lt;data_n, ray_t&gt;
  val pu = data.cartesian(rays).map { case ((n, data_vec), (t, ray_vec)) =&gt; ((n, t), data_vec dot ray_vec) }
  pu
}

val dst = compute_dst_sq()
val pu_train = compute_pu(train_data)
val pu_test = compute_pu(test_data)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_dst_sq: ()org.apache.spark.rdd.RDD[((Long, Long), Double)]
compute_pu: (data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])])org.apache.spark.rdd.RDD[((Long, Long), Double)]
dst: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[10492] at map at command-685894176423990:3
pu_train: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[10494] at map at command-685894176423990:9
pu_test: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[10496] at map at command-685894176423990:9
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Compute the lengths of all rays. The most expensive step.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_ray_lengths() = { // (M, T)
  // lengths[m, t, n] = dst[n, m] / (2 * (pu_train[n, t] - pu_test[m, t]))
  def compute_length(n: Long, m: Long, dst_val: Double, pu_train_val: Double, pu_test_val: Double) = {
    if (one_vs_all &amp;&amp; n == m) {
      Double.PositiveInfinity
    } else {
      val res = dst_val / (2 * (pu_train_val - pu_test_val))
      if (res &lt; 0) Double.PositiveInfinity else res
    }
  }
  
  def my_min(a: Double, b: Double) = {min(a, b)}
        
  val lengths = dst.cartesian(sc.range(0, T))
    .map { case (((n, m), dst_val), t) =&gt; ((n, t), (m, dst_val)) }  
    .join(pu_train) 
    .map { case ((n, t), ((m, dst_val), pu_train_val)) =&gt; ((m, t), (n, dst_val, pu_train_val)) }
    .join(pu_test) 
    .map { case ((m, t), ((n, dst_val, pu_train_val), pu_test_val)) =&gt; ((m, t), compute_length(n, m, dst_val, pu_train_val, pu_test_val)) } 
    .aggregateByKey(Double.PositiveInfinity)(my_min, my_min)  
  lengths
}

val lengths = compute_ray_lengths()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_ray_lengths: ()org.apache.spark.rdd.RDD[((Long, Long), Double)]
lengths: org.apache.spark.rdd.RDD[((Long, Long), Double)] = ShuffledRDD[10509] at aggregateByKey at command-685894176423991:20
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Compute the approximated weights.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_weights() = { // (M, )
  def agg_f(a: (Double, Double), b: (Double, Double)) = { (a._1 + b._1, a._2 + b._2) }
  
  val weights = lengths.map { case ((m, t), length) =&gt; (m, if (!length.isInfinity) (1.0, length) else (0.0, 0.0)) }
    .aggregateByKey((0.0, 0.0))(agg_f, agg_f)
    .map { case (m, (val1, val2)) =&gt; (m, if (val1 &gt; 0) val1 / val2 else 0.0) }
  weights
}

val weights = compute_weights()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_weights: ()org.apache.spark.rdd.RDD[(Long, Double)]
weights: org.apache.spark.rdd.RDD[(Long, Double)] = MapPartitionsRDD[10512] at map at command-685894176424002:6
</code></pre>
</div>
</div>
<div class="cell markdown">
<p>Save obtained data in csv.</p>
<p>Note: we repartition the tables here to work with one csv only; this should be removed for larger data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def save_data(name: String, data: RDD[(Long, DenseVector[Double])]) = {
  data.map { case (k, v) =&gt; k.toString() + &quot;,&quot; + v.toArray.mkString(&quot;,&quot;)}
    .toDF.repartition(1).write.format(&quot;csv&quot;).mode(SaveMode.Overwrite).option(&quot;quote&quot;, &quot; &quot;).save(&quot;dbfs:/FileStore/group17/data/&quot; + name)
}

def save_weights(name: String, data: RDD[(Long, Double)]) = {
  data.map { case (k, v) =&gt; k.toString() + &quot;,&quot; + v.toString}
    .toDF.repartition(1).write.format(&quot;csv&quot;).mode(SaveMode.Overwrite).option(&quot;quote&quot;, &quot; &quot;).save(&quot;dbfs:/FileStore/group17/data/&quot; + name)
}

save_data(&quot;gaussian_train&quot;, train_data)
save_data(&quot;gaussian_test&quot;, test_data)
save_weights(&quot;gaussian_weights&quot;, weights)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>save_data: (name: String, data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])])Unit
save_weights: (name: String, data: org.apache.spark.rdd.RDD[(Long, Double)])Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="gaussian-analysis"><a class="header" href="#gaussian-analysis">Gaussian Analysis</a></h1>
</div>
<div class="cell markdown">
<p>Here we test the results for normally distributed points.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">import numpy as np
import os
import shutil
import glob
import matplotlib.pyplot as plt
import scipy as sp
import scipy.stats as stats
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">os.listdir('/dbfs/FileStore/group17/data/')
</code></pre>
</div>
<div class="cell markdown">
<p>Reading the files.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def read_csv(data_name):
  results = glob.glob('/dbfs/FileStore/group17/data/' + data_name + '/*.csv')
  assert(len(results) == 1)
  filepath = results[0]
  
  csv = np.loadtxt(filepath, delimiter=',')
  csv = csv[csv[:, 0].argsort()]
  return csv
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">train_data = read_csv('gaussian_train')
test_data = read_csv('gaussian_test')
weights = read_csv('gaussian_weights')
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def display_density(data, weights):
  fig = plt.figure(figsize=(10, 10))
  plt.scatter(data[:, 0], data[:, 1], weights / np.max(weights) * 50)
  display(fig)
</code></pre>
</div>
<div class="cell markdown">
<p>True density visualization.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">true_density = stats.multivariate_normal.pdf(test_data[:, 1:], mean=np.zeros(2))

display_density(test_data[:, 1:], true_density)
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/17_02_1.JPG?raw=true" alt="" /></p>
</div>
<div class="cell markdown">
<p>Density, obtained from our method.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">display_density(test_data[:, 1:], weights[:, 1])
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/17_02_2.JPG?raw=true" alt="" /></p>
</div>
<div class="cell markdown">
<p>Density, obtained from kernel density estimation with tophat kernel.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">from sklearn.neighbors.kde import KernelDensity
kde = KernelDensity(kernel='tophat', bandwidth=0.13).fit(train_data[:, 1:])
kde_weights = kde.score_samples(test_data[:, 1:])
kde_weights = np.exp(kde_weights)

display_density(test_data[:, 1:], kde_weights)
</code></pre>
</div>
<div class="cell markdown">
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/17_02_3.JPG?raw=true" alt="" /></p>
</div>
<div class="cell markdown">
<p>Density, obtained from kernel density estimation with gaussian kernel.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">kde = KernelDensity(kernel='gaussian', bandwidth=0.13).fit(train_data[:, 1:])
gauss_weights = kde.score_samples(test_data[:, 1:])
gauss_weights = np.exp(kde_weights)

display_density(test_data[:, 1:], gauss_weights)
</code></pre>
</div>
<div class="cell markdown">
<p>A simple computation of the number of inverses.</p>
</div>
<div class="cell markdown">
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/17_02_4.JPG?raw=true" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">def rank_loss(a, b):
  n = a.shape[0]
  assert(n == b.shape[0])
  ans = 0
  for i in range(n):
    for j in range(i + 1, n):
      if (a[i] - a[j]) * (b[i] - b[j]) &lt; 0:
        ans += 1
  return ans
</code></pre>
</div>
<div class="cell markdown">
<p>Comparison of losses. On this one test, we get the smallest loss.</p>
<p>One of the immediate futher works: do a proper statistical comparison, also on different sizes of data.</p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">rank_loss(weights[:, 1], true_density)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">rank_loss(kde_weights, true_density)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">
rank_loss(gauss_weights, true_density)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
</div>
<div style="break-before: page; page-break-before: always;"></div><div class="cell markdown">
<p>ScaDaMaLe Course <a href="https://lamastex.github.io/scalable-data-science/sds/3/x/">site</a> and <a href="https://lamastex.github.io/ScaDaMaLe/index.html">book</a></p>
</div>
<div class="cell markdown">
<h1 id="robotics-dataset"><a class="header" href="#robotics-dataset">Robotics Dataset</a></h1>
<p>The estimation of probability density functions in a configuration space is of fundamental importance for many applications in probabilistic robotics and sampling-based robot motion planning.</p>
<p>Why is this application relevant? Standard sampling-based motion planners such as RRTConnect randomly explore the search space to iteratively build up a solution path. To speed up the planning it is common to use heuristics to explore the space in an informed way. The estimated densities are useful for designing such heurisitics. High-density regions might for example indicate narrow passages in the search space.</p>
<p>In the following we are going to apply the previously introduced algorithm to estimate densities of points in the configuration space of a multi-joint articulated robot arm. As shown in the figure below, the robot consists of 10 rotational joints and is placed in a 2-dim workspace. Several workspace obstacles are placed in the scene.</p>
<p>We ran RRT-Connect (a well-known shortest path motion planner) for different intial and goal configuration of the robot. The robot always starts in the right half of the workspace while the goal lies within one of the narrow passages as depicted in the figure.</p>
<p>To generate a dataset, we stitched all configurations from all planned paths together to one collection of joint configurations. Our goal is to estimate the density of robot configurations as generated by the RRTConnect motion planner. Originally, we used a dataset of ~125k points in 10 dimensions. Unfortunately, we observed that the current implementation does not scale well to such large dataset. For the scope of this project we instead run the method for 1000 points. Scaling up the implementation to larger dataset will be done in future work.</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/scene.png?raw=true" alt="" /></p>
<p>Here are some examples of paths found by the planner (red -&gt; final configuration, black -&gt; initial configuration, green-&gt; intermediate confgurations along the path):</p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/path_test_0.png?raw=true" alt="" /></p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/path_test_1.png?raw=true" alt="" /></p>
<p><img src="https://github.com/r-e-x-a-g-o-n/scalable-data-science/blob/master/images/ScaDaMaLe/000_0-sds-3-x-projects/path_test_2.png?raw=true" alt="" /></p>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python"># # Read data from file
# joint_centers_all = spark.read.format(&quot;csv&quot;).load(&quot;dbfs:/FileStore/shared_uploads/robert.gieselmann@gmail.com/robotics_dataset_joint_centers.csv&quot;,inferSchema =True,header=False)
# joint_centers_to_plot = np.reshape(np.array(joint_centers_all.collect()), (-1, 2))

# # Plot 2d histogram of joint center positions
# f = plt.figure()
# plt.hist2d(joint_centers_to_plot[:,0], joint_centers_to_plot[:,1], bins=250)
# plt.xlabel(&quot;x&quot;)
# plt.ylabel(&quot;y&quot;)
# plt.title(&quot;Distribution of joint center coordinates&quot;)

# # Display figure
# display()
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-python">#import os
#os.remove(&quot;/dbfs/FileStore/group17/data/robotics_test/joint_configs_test.csv&quot;)
#os.remove(&quot;/dbfs/FileStore/group17/data/robotics_train/joint_configs_train.csv&quot;)
#os.listdir(&quot;/dbfs/FileStore/group17/data/robotics_train&quot;)
#os.rename(&quot;/dbfs/FileStore/group17/data/robotics&quot;, &quot;/dbfs/FileStore/group17/data/robotics_train&quot;)
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.rdd.RDD
import breeze.linalg._
import breeze.numerics._
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.rdd.RDD
import breeze.linalg._
import breeze.numerics._
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Constants
val N = 1000 // train size
val M = 100 // test size
val D = 10 // dimensionality
val T = 100 // number of rays
val one_vs_all = true
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>N: Int = 1000
M: Int = 100
D: Int = 10
T: Int = 100
one_vs_all: Boolean = true
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Read the files from csv and convert to RDD
val df_train = spark.read.option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;false&quot;).format(&quot;csv&quot;).load(&quot;/FileStore/group17/data/robotics_train/joint_configs_train.csv&quot;)
val df_test = spark.read.option(&quot;inferSchema&quot;, &quot;true&quot;).option(&quot;header&quot;, &quot;false&quot;).format(&quot;csv&quot;).load(&quot;/FileStore/group17/data/robotics_test/joint_configs_test.csv&quot;)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>df_train: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 8 more fields]
df_test: org.apache.spark.sql.DataFrame = [_c0: double, _c1: double ... 8 more fields]
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">// Convert to RDD 
val rdd_train = df_train.rdd.map(_.toSeq.toArray.map(_.toString.toDouble))
val rdd_test = df_train.rdd.map(_.toSeq.toArray.map(_.toString.toDouble))

// Convert to Array[(Long, DenseVector)]
val train_data = rdd_train.zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
val test_data = if(one_vs_all) train_data else rdd_test.zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>rdd_train: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[2570] at map at command-3389902380791479:2
rdd_test: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[2571] at map at command-3389902380791479:3
train_data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[2573] at map at command-3389902380791479:6
test_data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[2573] at map at command-3389902380791479:6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">train_data.collect
</code></pre>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def get_uni_sphere() = {
  var u = RandomRDDs.normalVectorRDD(sc, T, D)
  u = new Normalizer().transform(u)
  var t = u.zipWithIndex().map { case (v, i) =&gt; (i, new DenseVector(v.toArray)) }
  t
}
  
val rays = get_uni_sphere()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>get_uni_sphere: ()org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])]
rays: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])] = MapPartitionsRDD[2577] at map at command-3389902380791431:4
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_dst_sq() = { // (N, M)
  // dst[n, m] = |x_n - x'_m|^2
  val dst = train_data.cartesian(test_data).map { case ((n, train_vec), (m, test_vec)) =&gt; ((n, m), sum(((train_vec - test_vec) *:* (train_vec - test_vec)) ^:^ 2.0) ) }
  dst
}

def compute_pu(data: RDD[(Long, DenseVector[Double])]) = { // (data.N, T)
  // pu[n, t] = &lt;data_n, ray_t&gt;
  val pu = data.cartesian(rays).map { case ((n, data_vec), (t, ray_vec)) =&gt; ((n, t), data_vec dot ray_vec) }
  pu
}

val dst = compute_dst_sq()
val pu_train = compute_pu(train_data)
val pu_test = compute_pu(test_data)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_dst_sq: ()org.apache.spark.rdd.RDD[((Long, Long), Double)]
compute_pu: (data: org.apache.spark.rdd.RDD[(Long, breeze.linalg.DenseVector[Double])])org.apache.spark.rdd.RDD[((Long, Long), Double)]
dst: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[2579] at map at command-3389902380791475:3
pu_train: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[2581] at map at command-3389902380791475:9
pu_test: org.apache.spark.rdd.RDD[((Long, Long), Double)] = MapPartitionsRDD[2583] at map at command-3389902380791475:9
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_ray_lengths() = { // (M, T)
  // lengths[m, t, n] = dst[n, m] / (2 * (pu_train[n, t] - pu_test[m, t]))
  def compute_length(n: Long, m: Long, dst_val: Double, pu_train_val: Double, pu_test_val: Double) = {
    if (one_vs_all &amp;&amp; n == m) {
      Double.PositiveInfinity
    } else {
      val res = dst_val / (2 * (pu_train_val - pu_test_val))
      if (res &lt; 0) Double.PositiveInfinity else res
    }
  }
  
  def my_min(a: Double, b: Double) = {min(a, b)}
        
  val lengths = dst.cartesian(sc.range(0, T))
    .map { case (((n, m), dst_val), t) =&gt; ((n, t), (m, dst_val)) }  
    .join(pu_train) 
    .map { case ((n, t), ((m, dst_val), pu_train_val)) =&gt; ((m, t), (n, dst_val, pu_train_val)) }
    .join(pu_test) 
    .map { case ((m, t), ((n, dst_val, pu_train_val), pu_test_val)) =&gt; ((m, t), compute_length(n, m, dst_val, pu_train_val, pu_test_val)) } 
    .aggregateByKey(Double.PositiveInfinity)(my_min, my_min)  
  lengths
}

val lengths = compute_ray_lengths()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_ray_lengths: ()org.apache.spark.rdd.RDD[((Long, Long), Double)]
lengths: org.apache.spark.rdd.RDD[((Long, Long), Double)] = ShuffledRDD[2596] at aggregateByKey at command-3389902380791476:20
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">def compute_weights() = { // (M, )
  def agg_f(a: (Double, Double), b: (Double, Double)) = { (a._1 + b._1, a._2 + b._2) }
  
  val weights = lengths.map { case ((m, t), length) =&gt; (m, if (!length.isInfinity) (1.0, length) else (0.0, 0.0)) }
    .aggregateByKey((0.0, 0.0))(agg_f, agg_f)
    .map { case (m, (val1, val2)) =&gt; (m, if (val1 &gt; 0) val1 / val2 else 0.0) }
  weights
}

val weights = compute_weights()
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>compute_weights: ()org.apache.spark.rdd.RDD[(Long, Double)]
weights: org.apache.spark.rdd.RDD[(Long, Double)] = MapPartitionsRDD[2599] at map at command-3389902380791477:6
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">//def save_data(name: String, data: RDD[(Long, DenseVector[Double])]) = {
//  data.map { case (k, v) =&gt; k.toString() + &quot;,&quot; + v.toArray.mkString(&quot;,&quot;)}
//    .toDF.repartition(1).write.format(&quot;csv&quot;).mode(SaveMode.Overwrite).option(&quot;quote&quot;, &quot; &quot;).save(&quot;dbfs:/FileStore/group17/data/&quot; + name)
//}

def save_weights(name: String, data: RDD[(Long, Double)]) = {
  data.map { case (k, v) =&gt; k.toString() + &quot;,&quot; + v.toString}
    .toDF.repartition(1).write.format(&quot;csv&quot;).mode(SaveMode.Overwrite).option(&quot;quote&quot;, &quot; &quot;).save(&quot;dbfs:/FileStore/group17/data/&quot; + name)
}

save_weights(&quot;robotics_weights&quot;, weights)
</code></pre>
<div class="output execute_result plain_result" execution_count="1">
<pre><code>save_weights: (name: String, data: org.apache.spark.rdd.RDD[(Long, Double)])Unit
</code></pre>
</div>
</div>
<div class="cell code" execution_count="1" scrolled="false">
<pre><code class="language-scala">weights.collect
</code></pre>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </body>
</html>
