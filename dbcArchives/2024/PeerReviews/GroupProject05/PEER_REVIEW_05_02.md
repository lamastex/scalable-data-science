# Peer-review of ScaDaMaLe Student Group Project Number Lukas Borggren

# Reviewer

- Sofia Andersson

# 1. Live/Video Presentation

Recall/watch the live/video presentation carefully and decide on a score between 0 and 2.

Choose one of the following options:

- 2 : The presentation was easily comprehensible and it helped me follow the project easily. I think I can definitely use the codes in the repository and adapt them if I were to encounter similar problems in the future, with possibly more in-depth self-study as needed.

# 2. Structure of Project Repository

The structure of the codes, including directory structure and coding/software-engineering practices,  were  

Choose one of the following options:

- 1 : Reasonably organised and could be improved (please consider making suggestions for improvement in Section 8 below) for easier comprehension.

# 3. Comments and explanations in code:

Choose one of the following options:

- 1 : There were minimal comments in the code but the markdown/documentation explained briefly what the code was supposed to be doing at least at a high level.

# 4. Originality or Difficulty of the Project

Choose one of the following options:

- 1 : The project is similar to an existing openly available project but significant contributions seem to be made in software versions, data used, explanations given and conclusions reached.

# 5. Scalability of the Project

Choose one of the following options:

- 2 : The project is truly implementing a *scalable data science process* and the same code can work with arbitrarily large input data if enough computing resources are available.

# 6. Total Grade

Add up all the scores from the above 5 Categories and report it below.

The Total Grade is: 7

# 7. Completing Peer-review Process

- Add this **Total Grade** as an integer to the LMS/Studium to complete your peer-review by the deadline.
- Attach you completed version of this file `PEER_REVIEW.md` to the LMS to complete the peer-review by the deadline.

# 8. Detailed Constructive Comments

Your README was very good when it comes to setup of the project, with quite detailed step-by-step instructions. There are plenty of links to all the resources used, which I appreciate a lot as someone who is not familiar with a lot of these cloud services. However, I would have liked to see a bit more information about the project itself, what you were trying to achieve and what the results were. I think that would have made it easier to understand the code and the project as a whole. I understand that you were fine-tuning an LLM on news data, but getting more information on why this is interesting and what it could be used for would have been useful.

I think the code was well-structured and easy to follow overall, but I would have liked to see more comments in the code, particularly in `main.py` where I was mostly confused on what was being done. Perhaps this is on me as I do not use a lot of the services that you do, but I struggled to understand how some of the code was connected to each other, especially those that used the Google Cloud services. However, if you use these services frequently, then the code might be quite easy to follow and this could definitely be on me! I do really like your function documentation though, as well as the recipe hack comment.

The structure of the repo is overall quite good, with just a few comments. I would have liked to see a bit more of a clear structure of the different parts of the project. At first I thought `preprocessing.py` was being used alongside `main.py` as they were in the same folder, but then when I looked, it appears as if they are fully separate. It would have been nice for clarity to have the different stages of the project to be more clearly separated in the repo.

Overall, very good job! A bit more background information in the README and perhaps some clarification on how the different parts of the code are connected would have been nice, but it's nothing huge and the project is interesting as is. Perhaps I would also emphasise how this is bringing something new to the table in the field of fine-tuning LLMs.
